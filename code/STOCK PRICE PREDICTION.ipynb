{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cd3132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import math\n",
    "import os\n",
    "import zipfile\n",
    "from datetime import datetime, timedelta\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27f32cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. DATA LOADING AND PREPROCESSING\n",
    "def load_stock_data(zip_file_path, ticker='AAPL', data_type='stocks'):\n",
    "    \"\"\"\n",
    "    Load stock data from Kaggle Stock Market Dataset zip file\n",
    "    \n",
    "    Parameters:\n",
    "    - zip_file_path: Path to the downloaded zip file\n",
    "    - ticker: Stock symbol (default: Apple)\n",
    "    - data_type: Either 'stocks' or 'etfs' (default: stocks)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with stock data\n",
    "    \"\"\"\n",
    "    print(f\"Loading {ticker} {data_type} data from zip file...\")\n",
    "    \n",
    "    # Check if the zip file exists\n",
    "    if not os.path.exists(zip_file_path):\n",
    "        print(f\"Zip file not found at {zip_file_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Construct the file path based on data type\n",
    "    if data_type.lower() not in ['stocks', 'etfs']:\n",
    "        print(f\"Invalid data_type: {data_type}. Must be 'stocks' or 'etfs'\")\n",
    "        return None\n",
    "    \n",
    "    ticker_file = f\"{data_type}/{ticker}.us.txt\"\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
    "            # Check if the ticker file exists in the zip\n",
    "            if ticker_file not in z.namelist():\n",
    "                print(f\"No data for ticker {ticker} in the {data_type} folder\")\n",
    "                return None\n",
    "            \n",
    "            # Read the CSV data for the ticker\n",
    "            with z.open(ticker_file) as f:\n",
    "                df = pd.read_csv(f, parse_dates=['Date'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Set Date as index\n",
    "    df.set_index('Date', inplace=True)\n",
    "    \n",
    "    print(f\"Loaded {len(df)} days of trading data for {ticker}\")\n",
    "    return df\n",
    "\n",
    "def get_ticker_metadata(zip_file_path, ticker=None):\n",
    "    \"\"\"\n",
    "    Get metadata for tickers from symbols_valid_meta.csv\n",
    "    \n",
    "    Parameters:\n",
    "    - zip_file_path: Path to the downloaded zip file\n",
    "    - ticker: Optional ticker to filter for (default: None, returns all)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with ticker metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
    "            meta_file = \"symbols_valid_meta.csv\"\n",
    "            if meta_file not in z.namelist():\n",
    "                print(f\"Metadata file not found in zip\")\n",
    "                return None\n",
    "                \n",
    "            with z.open(meta_file) as f:\n",
    "                meta_df = pd.read_csv(f)\n",
    "                \n",
    "            # If a specific ticker is requested, filter for it\n",
    "            if ticker:\n",
    "                meta_df = meta_df[meta_df['Symbol'] == ticker]\n",
    "                if meta_df.empty:\n",
    "                    print(f\"No metadata found for ticker {ticker}\")\n",
    "                    return None\n",
    "                    \n",
    "            return meta_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading metadata: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the stock data for modeling\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with stock data\n",
    "    \n",
    "    Returns:\n",
    "    - Processed DataFrame\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Check for expected columns based on the Kaggle dataset structure\n",
    "    expected_columns = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
    "    for col in expected_columns:\n",
    "        if col not in data.columns:\n",
    "            print(f\"Warning: Expected column '{col}' not found in data\")\n",
    "    \n",
    "    # Focus on the adjusted close price as our target (preferred over close for long-term analysis)\n",
    "    if 'Adj Close' in data.columns:\n",
    "        # Keep relevant columns\n",
    "        cols_to_keep = [col for col in expected_columns if col in data.columns]\n",
    "        data = data[cols_to_keep].copy()\n",
    "    elif 'Close' in data.columns:\n",
    "        # If Adj Close is not available, use Close\n",
    "        print(\"Note: Using 'Close' as target since 'Adj Close' is not available\")\n",
    "        cols_to_keep = [col for col in expected_columns if col in data.columns and col != 'Adj Close']\n",
    "        data = data[cols_to_keep].copy()\n",
    "    else:\n",
    "        raise ValueError(\"Neither 'Close' nor 'Adj Close' column found in data\")\n",
    "    \n",
    "    # Check for and handle missing values\n",
    "    if data.isnull().sum().sum() > 0:\n",
    "        print(f\"Found {data.isnull().sum().sum()} missing values. Filling with forward fill method.\")\n",
    "        data = data.fillna(method='ffill')\n",
    "        # If there are still NAs at the beginning, use backward fill\n",
    "        data = data.fillna(method='bfill')\n",
    "    \n",
    "    # Add date-based features\n",
    "    data['Date'] = data.index\n",
    "    data['DayOfWeek'] = data['Date'].dt.dayofweek\n",
    "    data['Month'] = data['Date'].dt.month\n",
    "    data['Year'] = data['Date'].dt.year\n",
    "    data['DayOfYear'] = data['Date'].dt.dayofyear\n",
    "    \n",
    "    # Define target column based on availability\n",
    "    target_col = 'Adj Close' if 'Adj Close' in data.columns else 'Close'\n",
    "    \n",
    "    # Calculate technical indicators\n",
    "    # Moving averages\n",
    "    data['MA5'] = data[target_col].rolling(window=5).mean()\n",
    "    data['MA20'] = data[target_col].rolling(window=20).mean()\n",
    "    data['MA50'] = data[target_col].rolling(window=50).mean()\n",
    "    \n",
    "    # Exponential moving averages\n",
    "    data['EMA12'] = data[target_col].ewm(span=12, adjust=False).mean()\n",
    "    data['EMA26'] = data[target_col].ewm(span=26, adjust=False).mean()\n",
    "    \n",
    "    # MACD (Moving Average Convergence Divergence)\n",
    "    data['MACD'] = data['EMA12'] - data['EMA26']\n",
    "    data['MACD_signal'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    data['BB_middle'] = data[target_col].rolling(window=20).mean()\n",
    "    data['BB_std'] = data[target_col].rolling(window=20).std()\n",
    "    data['BB_upper'] = data['BB_middle'] + (data['BB_std'] * 2)\n",
    "    data['BB_lower'] = data['BB_middle'] - (data['BB_std'] * 2)\n",
    "    data['BB_width'] = (data['BB_upper'] - data['BB_lower']) / data['BB_middle']\n",
    "    \n",
    "    # RSI (Relative Strength Index)\n",
    "    delta = data[target_col].diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(window=14).mean()\n",
    "    loss = -delta.where(delta < 0, 0).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    data['RSI'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Calculate volatility (standard deviation over the past 20 days)\n",
    "    data['Volatility'] = data[target_col].rolling(window=20).std()\n",
    "    \n",
    "    # Calculate percentage change\n",
    "    data['Returns'] = data[target_col].pct_change()\n",
    "    \n",
    "    # Add price differences\n",
    "    if all(col in data.columns for col in ['Open', 'High', 'Low', 'Close']):\n",
    "        data['PriceRange'] = data['High'] - data['Low']\n",
    "        data['DailyChange'] = data['Close'] - data['Open']\n",
    "        data['CloseRatio'] = data['Close'] / data['Open']\n",
    "        data['HL_PCT'] = (data['High'] - data['Low']) / data['Close'] * 100\n",
    "        data['PCT_change'] = (data['Close'] - data['Open']) / data['Open'] * 100\n",
    "    \n",
    "    # Add volume indicators if volume data is available\n",
    "    if 'Volume' in data.columns:\n",
    "        data['VolumeSMA5'] = data['Volume'].rolling(window=5).mean()\n",
    "        data['VolumeChange'] = data['Volume'].pct_change()\n",
    "        # Price-volume trend\n",
    "        data['PVT'] = (data[target_col].pct_change() * data['Volume']).cumsum()\n",
    "    \n",
    "    # Drop rows with NaN values (first rows due to rolling calculations)\n",
    "    data = data.dropna()\n",
    "    \n",
    "    # Store the date index before any reset_index operations\n",
    "    data['DateIndex'] = data.index\n",
    "    \n",
    "    return data, target_col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb3e65bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. FEATURE ENGINEERING FOR TIME SERIES\n",
    "\n",
    "def create_sequences(data, n_steps):\n",
    "    \"\"\"\n",
    "    Create sequences for time series prediction\n",
    "    \n",
    "    Parameters:\n",
    "    - data: Scaled data array\n",
    "    - n_steps: Number of time steps to use as input features\n",
    "    \n",
    "    Returns:\n",
    "    - X: Input sequences\n",
    "    - y: Target values\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps):\n",
    "        X.append(data[i:(i + n_steps), 0])\n",
    "        y.append(data[i + n_steps, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def prepare_lstm_data(data, target_col='Close', n_steps=60, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Prepare data for LSTM model\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame with stock data\n",
    "    - target_col: Target column to predict\n",
    "    - n_steps: Number of past time steps to use as features\n",
    "    - test_size: Proportion of data to use for testing\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary containing all prepared data and scalers\n",
    "    \"\"\"\n",
    "    # Extract the target column\n",
    "    dataset = data[target_col].values.reshape(-1, 1)\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataset_scaled = scaler.fit_transform(dataset)\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = create_sequences(dataset_scaled, n_steps)\n",
    "    \n",
    "    # Reshape X to the format [samples, time steps, features]\n",
    "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    split_idx = int(len(X) * (1 - test_size))\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    # Get corresponding dates for plotting\n",
    "    dates = data['DateIndex'].values[n_steps:]\n",
    "    train_dates = dates[:split_idx]\n",
    "    test_dates = dates[split_idx:]\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'scaler': scaler,\n",
    "        'train_dates': train_dates,\n",
    "        'test_dates': test_dates,\n",
    "        'all_dates': dates,\n",
    "        'n_steps': n_steps\n",
    "    }\n",
    "\n",
    "def prepare_traditional_ml_data(data, target_col='Close', test_size=0.2):\n",
    "    \"\"\"\n",
    "    Prepare data for traditional ML models\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame with stock data\n",
    "    - target_col: Target column to predict\n",
    "    - test_size: Proportion of data to use for testing\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary containing prepared data\n",
    "    \"\"\"\n",
    "    # Define features and target\n",
    "    feature_cols = [col for col in data.columns if col not in [target_col, 'Date', 'DateIndex']]\n",
    "    X = data[feature_cols].values\n",
    "    y = data[target_col].values\n",
    "    \n",
    "    # Scale features\n",
    "    scaler_X = MinMaxScaler()\n",
    "    X_scaled = scaler_X.fit_transform(X)\n",
    "    \n",
    "    # Scale target\n",
    "    scaler_y = MinMaxScaler()\n",
    "    y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_scaled, test_size=test_size, shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Get corresponding dates for plotting\n",
    "    dates = data['DateIndex'].values\n",
    "    split_idx = len(X_train)\n",
    "    train_dates = dates[:split_idx]\n",
    "    test_dates = dates[split_idx:]\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'scaler_X': scaler_X,\n",
    "        'scaler_y': scaler_y,\n",
    "        'train_dates': train_dates,\n",
    "        'test_dates': test_dates,\n",
    "        'feature_cols': feature_cols\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad4b390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. MODEL DEVELOPMENT\n",
    "\n",
    "def build_lstm_model(input_shape):\n",
    "    \"\"\"\n",
    "    Build an LSTM model for time series prediction\n",
    "    \n",
    "    Parameters:\n",
    "    - input_shape: Shape of input data (time steps, features)\n",
    "    \n",
    "    Returns:\n",
    "    - Compiled LSTM model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First LSTM layer with Dropout\n",
    "    model.add(LSTM(units=50, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Second LSTM layer with Dropout\n",
    "    model.add(LSTM(units=50, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Dense output layer\n",
    "    model.add(Dense(units=1))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_lstm_model(prepared_data, epochs=50, batch_size=32, verbose=1):\n",
    "    \"\"\"\n",
    "    Train the LSTM model\n",
    "    \n",
    "    Parameters:\n",
    "    - prepared_data: Dictionary with prepared data from prepare_lstm_data()\n",
    "    - epochs: Number of training epochs\n",
    "    - batch_size: Training batch size\n",
    "    - verbose: Verbosity mode\n",
    "    \n",
    "    Returns:\n",
    "    - Trained model and history\n",
    "    \"\"\"\n",
    "    # Extract data\n",
    "    X_train = prepared_data['X_train']\n",
    "    y_train = prepared_data['y_train']\n",
    "    X_test = prepared_data['X_test']\n",
    "    y_test = prepared_data['y_test']\n",
    "    \n",
    "    # Define input shape\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    \n",
    "    # Build model\n",
    "    model = build_lstm_model(input_shape)\n",
    "    \n",
    "    # Define early stopping\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stop],\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def train_random_forest(prepared_data, n_estimators=100, verbose=1):\n",
    "    \"\"\"\n",
    "    Train a Random Forest model\n",
    "    \n",
    "    Parameters:\n",
    "    - prepared_data: Dictionary with prepared data from prepare_traditional_ml_data()\n",
    "    - n_estimators: Number of trees in the forest\n",
    "    - verbose: Verbosity mode\n",
    "    \n",
    "    Returns:\n",
    "    - Trained model\n",
    "    \"\"\"\n",
    "    # Extract data\n",
    "    X_train = prepared_data['X_train']\n",
    "    y_train = prepared_data['y_train']\n",
    "    \n",
    "    # Build and train model\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators, random_state=42)\n",
    "    if verbose:\n",
    "        print(\"Training Random Forest model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    if verbose:\n",
    "        print(\"Training completed!\")\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06916cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. MODEL EVALUATION\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Evaluate model performance\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: True values\n",
    "    - y_pred: Predicted values\n",
    "    - model_name: Name of the model for display\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = math.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance Metrics:\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2\n",
    "    }\n",
    "\n",
    "def predict_lstm(model, prepared_data):\n",
    "    \"\"\"\n",
    "    Make predictions using the LSTM model\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained LSTM model\n",
    "    - prepared_data: Dictionary with prepared data\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with predictions and true values\n",
    "    \"\"\"\n",
    "    # Extract data\n",
    "    X_train = prepared_data['X_train']\n",
    "    y_train = prepared_data['y_train']\n",
    "    X_test = prepared_data['X_test']\n",
    "    y_test = prepared_data['y_test']\n",
    "    scaler = prepared_data['scaler']\n",
    "    \n",
    "    # Make predictions\n",
    "    train_predict = model.predict(X_train)\n",
    "    test_predict = model.predict(X_test)\n",
    "    \n",
    "    # Inverse transform predictions\n",
    "    train_predict = scaler.inverse_transform(train_predict)\n",
    "    test_predict = scaler.inverse_transform(test_predict)\n",
    "    \n",
    "    # Inverse transform true values\n",
    "    y_train_inv = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "    y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    return {\n",
    "        'train_predict': train_predict.flatten(),\n",
    "        'test_predict': test_predict.flatten(),\n",
    "        'y_train': y_train_inv.flatten(),\n",
    "        'y_test': y_test_inv.flatten()\n",
    "    }\n",
    "\n",
    "def predict_rf(model, prepared_data):\n",
    "    \"\"\"\n",
    "    Make predictions using the Random Forest model\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained Random Forest model\n",
    "    - prepared_data: Dictionary with prepared data\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with predictions and true values\n",
    "    \"\"\"\n",
    "    # Extract data\n",
    "    X_train = prepared_data['X_train']\n",
    "    y_train = prepared_data['y_train']\n",
    "    X_test = prepared_data['X_test']\n",
    "    y_test = prepared_data['y_test']\n",
    "    scaler_y = prepared_data['scaler_y']\n",
    "    \n",
    "    # Make predictions\n",
    "    train_predict_scaled = model.predict(X_train)\n",
    "    test_predict_scaled = model.predict(X_test)\n",
    "    \n",
    "    # Reshape for inverse transform\n",
    "    train_predict_scaled = train_predict_scaled.reshape(-1, 1)\n",
    "    test_predict_scaled = test_predict_scaled.reshape(-1, 1)\n",
    "    y_train_reshaped = y_train.reshape(-1, 1)\n",
    "    y_test_reshaped = y_test.reshape(-1, 1)\n",
    "    \n",
    "    # Inverse transform predictions and true values\n",
    "    train_predict = scaler_y.inverse_transform(train_predict_scaled).flatten()\n",
    "    test_predict = scaler_y.inverse_transform(test_predict_scaled).flatten()\n",
    "    y_train_inv = scaler_y.inverse_transform(y_train_reshaped).flatten()\n",
    "    y_test_inv = scaler_y.inverse_transform(y_test_reshaped).flatten()\n",
    "    \n",
    "    return {\n",
    "        'train_predict': train_predict,\n",
    "        'test_predict': test_predict,\n",
    "        'y_train': y_train_inv,\n",
    "        'y_test': y_test_inv\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "812c1ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. VISUALIZATION\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot LSTM training history\n",
    "    \n",
    "    Parameters:\n",
    "    - history: History object from model.fit()\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss During Training')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_predictions(predictions, dates, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Plot model predictions against actual values\n",
    "    \n",
    "    Parameters:\n",
    "    - predictions: Dictionary with predictions from predict_lstm() or predict_rf()\n",
    "    - dates: Dictionary with dates from prepared data\n",
    "    - model_name: Name of the model for display\n",
    "    \"\"\"\n",
    "    train_dates = dates['train_dates']\n",
    "    test_dates = dates['test_dates']\n",
    "    \n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    # Plot training data\n",
    "    plt.plot(train_dates, predictions['y_train'], label='Actual (Training)')\n",
    "    plt.plot(train_dates, predictions['train_predict'], label='Predicted (Training)')\n",
    "    \n",
    "    # Plot testing data\n",
    "    plt.plot(test_dates, predictions['y_test'], label='Actual (Testing)', color='green')\n",
    "    plt.plot(test_dates, predictions['test_predict'], label='Predicted (Testing)', color='red')\n",
    "    \n",
    "    # Add vertical line to separate training and testing periods\n",
    "    split_date = test_dates[0]\n",
    "    plt.axvline(x=split_date, color='black', linestyle='--')\n",
    "    plt.text(split_date, plt.ylim()[1]*0.9, 'Train/Test Split', \n",
    "             horizontalalignment='center', backgroundcolor='white')\n",
    "    \n",
    "    plt.title(f'{model_name} - Stock Price Prediction')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Stock Price')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_multiple_predictions(predictions_list, dates, model_names):\n",
    "    \"\"\"\n",
    "    Plot predictions from multiple models for comparison\n",
    "    \n",
    "    Parameters:\n",
    "    - predictions_list: List of prediction dictionaries\n",
    "    - dates: Dictionary with dates from prepared data\n",
    "    - model_names: List of model names\n",
    "    \"\"\"\n",
    "    test_dates = dates['test_dates']\n",
    "    \n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    # Plot actual test data\n",
    "    plt.plot(test_dates, predictions_list[0]['y_test'], label='Actual', linewidth=2)\n",
    "    \n",
    "    # Plot predictions from each model\n",
    "    for i, predictions in enumerate(predictions_list):\n",
    "        plt.plot(test_dates, predictions['test_predict'], label=f'Predicted ({model_names[i]})', linestyle='--')\n",
    "    \n",
    "    plt.title('Model Comparison - Stock Price Prediction')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Stock Price')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_forecast(last_sequence, model, scaler, n_steps, n_forecast=30, actual_data=None):\n",
    "    \"\"\"\n",
    "    Generate and plot future stock price forecast\n",
    "    \n",
    "    Parameters:\n",
    "    - last_sequence: Last sequence from the original data\n",
    "    - model: Trained LSTM model\n",
    "    - scaler: Scaler used on the original data\n",
    "    - n_steps: Number of time steps used in sequences\n",
    "    - n_forecast: Number of days to forecast\n",
    "    - actual_data: Actual future data for comparison (if available)\n",
    "    \"\"\"\n",
    "    # Make a copy of the last sequence\n",
    "    curr_sequence = last_sequence.copy()\n",
    "    \n",
    "    # List to store forecasted values\n",
    "    forecasted_values = []\n",
    "    \n",
    "    # Generate forecasts\n",
    "    for _ in range(n_forecast):\n",
    "        # Get prediction (scaled)\n",
    "        pred = model.predict(curr_sequence.reshape(1, n_steps, 1))\n",
    "        \n",
    "        # Append prediction to the sequence and remove the first element\n",
    "        curr_sequence = np.append(curr_sequence[1:], pred[0])\n",
    "        \n",
    "        # Store the forecasted value (unscaled)\n",
    "        forecasted_values.append(scaler.inverse_transform([[pred[0, 0]]])[0, 0])\n",
    "    \n",
    "    # Generate dates for the forecast period\n",
    "    last_date = datetime.now()\n",
    "    forecast_dates = [last_date + timedelta(days=i) for i in range(1, n_forecast + 1)]\n",
    "    \n",
    "    # Plot the forecast\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    # Plot actual data if available\n",
    "    if actual_data is not None:\n",
    "        plt.plot(actual_data['dates'], actual_data['values'], label='Actual', color='green')\n",
    "    \n",
    "    # Plot forecasted data\n",
    "    plt.plot(forecast_dates, forecasted_values, label='Forecast', color='red', linestyle='--')\n",
    "    \n",
    "    plt.title('Stock Price Forecast')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Stock Price')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return forecasted_values, forecast_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8058bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting stock price prediction for AAPL using Kaggle Stock Market Dataset\n",
      "Loading AAPL stocks data from zip file...\n",
      "No data for ticker AAPL in the stocks folder\n",
      "Failed to load data. Listing available tickers...\n",
      "Found 0 stock tickers\n",
      "Found 0 ETF tickers\n",
      "Metadata file found: symbols_valid_meta.csv\n",
      "{'stocks': [], 'etfs': [], 'has_metadata': True}\n",
      "\n",
      "Please choose one of the available tickers and try again.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6. MAIN EXECUTION\n",
    "\n",
    "def list_available_tickers(zip_file_path, data_type='both', limit=20):\n",
    "    \"\"\"\n",
    "    List available ticker symbols in the dataset\n",
    "    \n",
    "    Parameters:\n",
    "    - zip_file_path: Path to the downloaded zip file\n",
    "    - data_type: Type of data to list - 'stocks', 'etfs', or 'both' (default)\n",
    "    - limit: Maximum number of tickers to list per type\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with available tickers by type\n",
    "    \"\"\"\n",
    "    if not os.path.exists(zip_file_path):\n",
    "        print(f\"Zip file not found at {zip_file_path}\")\n",
    "        return {}\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
    "            # Process stocks if requested\n",
    "            if data_type.lower() in ['stocks', 'both']:\n",
    "                stock_tickers = [name.split('/')[-1].split('.us.txt')[0] for name in z.namelist() \n",
    "                               if name.startswith('stocks/') and name.endswith('.us.txt')]\n",
    "                stock_tickers.sort()\n",
    "                result['stocks'] = stock_tickers\n",
    "                \n",
    "                if limit and len(stock_tickers) > limit:\n",
    "                    print(f\"Found {len(stock_tickers)} stock tickers. Showing first {limit}:\")\n",
    "                    for ticker in stock_tickers[:limit]:\n",
    "                        print(f\"  - {ticker}\")\n",
    "                    print(f\"  ... and {len(stock_tickers) - limit} more\")\n",
    "                else:\n",
    "                    print(f\"Found {len(stock_tickers)} stock tickers\")\n",
    "            \n",
    "            # Process ETFs if requested\n",
    "            if data_type.lower() in ['etfs', 'both']:\n",
    "                etf_tickers = [name.split('/')[-1].split('.us.txt')[0] for name in z.namelist() \n",
    "                             if name.startswith('etfs/') and name.endswith('.us.txt')]\n",
    "                etf_tickers.sort()\n",
    "                result['etfs'] = etf_tickers\n",
    "                \n",
    "                if limit and len(etf_tickers) > limit:\n",
    "                    print(f\"Found {len(etf_tickers)} ETF tickers. Showing first {limit}:\")\n",
    "                    for ticker in etf_tickers[:limit]:\n",
    "                        print(f\"  - {ticker}\")\n",
    "                    print(f\"  ... and {len(etf_tickers) - limit} more\")\n",
    "                else:\n",
    "                    print(f\"Found {len(etf_tickers)} ETF tickers\")\n",
    "            \n",
    "            # Try to get metadata file\n",
    "            meta_file = \"symbols_valid_meta.csv\"\n",
    "            if meta_file in z.namelist():\n",
    "                print(f\"Metadata file found: symbols_valid_meta.csv\")\n",
    "                result['has_metadata'] = True\n",
    "            else:\n",
    "                print(f\"Metadata file not found\")\n",
    "                result['has_metadata'] = False\n",
    "                \n",
    "            return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing tickers: {e}\")\n",
    "        return {}\n",
    "\n",
    "def main(zip_file_path=\"../data/STOCK_PRICE_PREDICTION.zip\", ticker=\"AAPL\"):\n",
    "    \"\"\"\n",
    "    Main execution function\n",
    "    \n",
    "    Parameters:\n",
    "    - zip_file_path: Path to the downloaded zip file\n",
    "    - ticker: Stock symbol to analyze\n",
    "    \"\"\"\n",
    "    print(f\"Starting stock price prediction for {ticker} using Kaggle Stock Market Dataset\")\n",
    "    \n",
    "    # 1. Load stock data\n",
    "    df = load_stock_data(zip_file_path, ticker=ticker)\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"Failed to load data. Listing available tickers...\")\n",
    "        available_tickers = list_available_tickers(zip_file_path)\n",
    "        print(available_tickers)\n",
    "        print(\"\\nPlease choose one of the available tickers and try again.\")\n",
    "        return\n",
    "    \n",
    "    # Basic info about the data\n",
    "    print(\"\\nData Information:\")\n",
    "    print(df.info())\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # 2. Preprocess data\n",
    "    print(\"\\nPreprocessing data...\")\n",
    "    processed_data = preprocess_data(df)\n",
    "    \n",
    "    # 3. Prepare data for models\n",
    "    print(\"\\nPreparing data for LSTM model...\")\n",
    "    lstm_data = prepare_lstm_data(processed_data, n_steps=60)\n",
    "    \n",
    "    print(\"\\nPreparing data for traditional ML models...\")\n",
    "    trad_data = prepare_traditional_ml_data(processed_data)\n",
    "    \n",
    "    # 4. Train LSTM model\n",
    "    print(\"\\nTraining LSTM model...\")\n",
    "    lstm_model, history = train_lstm_model(lstm_data, epochs=50)\n",
    "    \n",
    "    # 5. Train Random Forest model\n",
    "    print(\"\\nTraining Random Forest model...\")\n",
    "    rf_model = train_random_forest(trad_data)\n",
    "    \n",
    "    # 6. Make predictions\n",
    "    print(\"\\nMaking predictions with LSTM model...\")\n",
    "    lstm_predictions = predict_lstm(lstm_model, lstm_data)\n",
    "    \n",
    "    print(\"\\nMaking predictions with Random Forest model...\")\n",
    "    rf_predictions = predict_rf(rf_model, trad_data)\n",
    "    \n",
    "    # 7. Evaluate models\n",
    "    print(\"\\nEvaluating LSTM model...\")\n",
    "    lstm_metrics = evaluate_model(lstm_predictions['y_test'], lstm_predictions['test_predict'], \"LSTM\")\n",
    "    \n",
    "    print(\"\\nEvaluating Random Forest model...\")\n",
    "    rf_metrics = evaluate_model(rf_predictions['y_test'], rf_predictions['test_predict'], \"Random Forest\")\n",
    "    \n",
    "    # 8. Plot results\n",
    "    print(\"\\nPlotting training history...\")\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    print(\"\\nPlotting LSTM predictions...\")\n",
    "    lstm_dates = {'train_dates': lstm_data['train_dates'], 'test_dates': lstm_data['test_dates']}\n",
    "    plot_predictions(lstm_predictions, lstm_dates, \"LSTM\")\n",
    "    \n",
    "    print(\"\\nPlotting Random Forest predictions...\")\n",
    "    rf_dates = {'train_dates': trad_data['train_dates'], 'test_dates': trad_data['test_dates']}\n",
    "    plot_predictions(rf_predictions, rf_dates, \"Random Forest\")\n",
    "    \n",
    "    print(\"\\nPlotting model comparison...\")\n",
    "    model_names = [\"LSTM\", \"Random Forest\"]\n",
    "    predictions_list = [lstm_predictions, rf_predictions]\n",
    "    dates = {'test_dates': lstm_data['test_dates']}  # Assuming same test dates for both\n",
    "    plot_multiple_predictions(predictions_list, dates, model_names)\n",
    "    \n",
    "    # 9. Generate future forecast (next 30 days)\n",
    "    print(\"\\nGenerating future forecast with LSTM model...\")\n",
    "    # Last sequence from the test data\n",
    "    last_sequence = lstm_data['X_test'][-1]\n",
    "    last_date = lstm_data['test_dates'][-1]\n",
    "    forecasted_values, forecast_dates = plot_forecast(last_sequence, lstm_model, lstm_data['scaler'], lstm_data['n_steps'], n_forecast=30)\n",
    "    \n",
    "    print(f\"\\nForecasted stock prices for {ticker} for the next 30 days after {last_date.strftime('%Y-%m-%d')}:\")\n",
    "    for i, (date, price) in enumerate(zip(forecast_dates, forecasted_values), 1):\n",
    "        print(f\"Day {i}: {date.strftime('%Y-%m-%d')} - ${price:.2f}\")\n",
    "    \n",
    "    # 10. Feature importance from Random Forest (for insights)\n",
    "    if hasattr(rf_model, 'feature_importances_'):\n",
    "        print(\"\\nFeature Importances from Random Forest:\")\n",
    "        feature_cols = trad_data['feature_cols']\n",
    "        importances = rf_model.feature_importances_\n",
    "        feature_importance = pd.DataFrame({'Feature': feature_cols, 'Importance': importances})\n",
    "        feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "        print(feature_importance.head(10))\n",
    "        \n",
    "        # Plot feature importances\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.barh(feature_importance['Feature'].head(10), feature_importance['Importance'].head(10))\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title(f'Top 10 Feature Importances for {ticker} Stock Price Prediction')\n",
    "        plt.gca().invert_yaxis()  # Invert to have the most important at the top\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\nAnalysis complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # You need to download the stock market dataset from Kaggle:\n",
    "    # https://www.kaggle.com/datasets/jacksoncrow/stock-market-dataset\n",
    "    # And provide the path to the downloaded zip file\n",
    "    \n",
    "    # Example usage:\n",
    "    # 1. Just specify the path to the downloaded zip file (will use AAPL by default)\n",
    "    # main(\"path/to/stock_market_data.zip\")\n",
    "    \n",
    "    # 2. Specify both the path and ticker\n",
    "    # main(\"path/to/stock_market_data.zip\", \"MSFT\")\n",
    "    \n",
    "    # To list available tickers in the dataset:\n",
    "    # list_available_tickers(\"path/to/stock_market_data.zip\")\n",
    "    \n",
    "    # Replace with your actual file path\n",
    "    # For example:\n",
    "    # main(\"./stock_market_data.zip\", \"AAPL\")\n",
    "    \n",
    "    # print(\"Please update the script with the path to your downloaded dataset zip file.\")\n",
    "    # print(\"Example usage: main('./stock_market_data.zip', 'AAPL')\")\n",
    "    \n",
    "    # Uncomment and update the line below with your local file path\n",
    "    main(\"../data/STOCK_PRICE_PREDICTION.zip\", \"AAPL\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
